# -*- coding: utf-8 -*-
"""Prodigy_ML_04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19AsZ5shXLUjj-tXdLDHf4axA6iMJjPRV
"""

# 🖐️ Hand Gesture Recognition using CNN (Image + Video Support)


import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from google.colab.patches import cv2_imshow

# --------------------------------------------------------------
# 1. Dataset Setup
# --------------------------------------------------------------
# Expected folder structure:
# /content/gestures/
#     ├── fist/
#     ├── palm/
#     ├── thumbs_up/
#     └── ok/
# (you can add more gesture classes as needed)

DATASET_DIR = "/content/gestures"
IMG_SIZE = 64

def load_dataset(dataset_dir):
    data = []
    labels = []
    class_names = sorted(os.listdir(dataset_dir))
    label_map = {class_name: idx for idx, class_name in enumerate(class_names)}

    for class_name in class_names:
        class_dir = os.path.join(dataset_dir, class_name)
        for img_name in os.listdir(class_dir):
            img_path = os.path.join(class_dir, img_name)
            try:
                img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))
                img_array = img_to_array(img) / 255.0
                data.append(img_array)
                labels.append(label_map[class_name])
            except Exception as e:
                print(f"Error loading {img_path}: {e}")

    return np.array(data), to_categorical(np.array(labels)), class_names

print("==> Loading dataset...")
X, y, class_names = load_dataset(DATASET_DIR)
print(f"Dataset loaded: {X.shape}, Labels: {y.shape}, Classes: {class_names}")

# --------------------------------------------------------------
# 2. Train/Test Split
# --------------------------------------------------------------
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# --------------------------------------------------------------
# 3. Data Augmentation
# --------------------------------------------------------------
datagen = ImageDataGenerator(
    rotation_range=20,
    zoom_range=0.2,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)
datagen.fit(X_train)

# --------------------------------------------------------------
# 4. Model Building (CNN)
# --------------------------------------------------------------
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(class_names), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# --------------------------------------------------------------
# 5. Training
# --------------------------------------------------------------
EPOCHS = 10
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=EPOCHS,
    validation_data=(X_val, y_val)
)

# --------------------------------------------------------------
# 6. Evaluate
# --------------------------------------------------------------
loss, acc = model.evaluate(X_val, y_val)
print(f"✅ Validation Accuracy: {acc*100:.2f}%")

# --------------------------------------------------------------
# 7. Test Prediction on an Image
# --------------------------------------------------------------
def predict_gesture(img_path):
    img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)
    prediction = model.predict(img_array)[0]
    predicted_class = class_names[np.argmax(prediction)]
    confidence = np.max(prediction)

    print(f"Prediction: {predicted_class} ({confidence*100:.2f}%)")
    plt.imshow(load_img(img_path))
    plt.title(f"{predicted_class} ({confidence*100:.2f}%)")
    plt.axis("off")
    plt.show()

# Example: predict_gesture("/content/gestures/palm/sample1.jpg")

# --------------------------------------------------------------
# 8. Real-Time Video Gesture Recognition (Webcam)
# --------------------------------------------------------------
def recognize_from_video(video_path=None):
    if video_path:
        cap = cv2.VideoCapture(video_path)
    else:
        cap = cv2.VideoCapture(0)  # Webcam

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Preprocess frame
        img = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))
        img_array = img.astype("float32") / 255.0
        img_array = np.expand_dims(img_array, axis=0)

        # Prediction
        prediction = model.predict(img_array)[0]
        predicted_class = class_names[np.argmax(prediction)]
        confidence = np.max(prediction)

        # Overlay result
        cv2.putText(frame, f"{predicted_class} ({confidence*100:.1f}%)",
                    (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)

        cv2_imshow(frame)  # For Colab
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# Example: recognize_from_video("/content/sample_video.mp4")
# Or webcam: recognize_from_video()